¡Claro que sí! Aquí tienes el documento completo "Chord: A Scalable Peer-to-peer Lookup Service for Internet Applications" traducido al español.  He hecho mi mejor esfuerzo para mantener la precisión y el sentido del texto original, pero algunas frases podrían ser algo literales debido a la naturaleza técnica del documento. 

---

## Chord: Un Servicio de Búsqueda Escalable de Par a Par para Aplicaciones de Internet

**Ion Stoica**, **Robert Morris**, **David Karger**, **M. Frans Kaashoek**, **Hari Balakrishnan**

MIT Laboratory for Computer Science chord@lcs.mit.edu

http://pdos.lcs.mit.edu/chord/

**Resumen**

Un problema fundamental que enfrentan las aplicaciones de par a par es la ubicación eficiente del nodo que almacena un elemento de datos particular. Este documento presenta Chord, un protocolo de búsqueda distribuida que aborda este problema. Chord proporciona soporte para solo una operación: dado una clave, mapea la clave a un nodo. La ubicación de datos se puede implementar fácilmente encima de Chord asociando una clave con cada elemento de datos y almacenando el par clave/elemento de datos en el nodo al que se mapea la clave. Chord se adapta eficientemente a medida que los nodos se unen y se van del sistema, y puede responder consultas incluso si el sistema está cambiando continuamente. Los resultados del análisis teórico, las simulaciones y los experimentos muestran que Chord es escalable, con un costo de comunicación y el estado mantenido por cada nodo que escalan logarítmicamente con la cantidad de nodos Chord.

**1. Introducción**

Los sistemas y aplicaciones de par a par son sistemas distribuidos sin control centralizado u organización jerárquica, donde el software que se ejecuta en cada nodo es equivalente en funcionalidad. Una revisión de las características de las aplicaciones de par a par recientes arroja una larga lista: almacenamiento redundante, permanencia, selección de servidores cercanos, anonimato, búsqueda, autenticación y nomenclatura jerárquica. A pesar de este rico conjunto de características, la operación central en la mayoría de los sistemas de par a par es la ubicación eficiente de los elementos de datos. La contribución de este documento es un protocolo escalable para la búsqueda en un sistema de par a par dinámico con frecuentes llegadas y salidas de nodos.

El protocolo Chord admite solo una operación: dado una clave, mapea la clave a un nodo. Dependiendo de la aplicación que utiliza Chord, ese nodo podría ser responsable de almacenar un valor asociado con la clave. Chord utiliza una variante de hashing consistente [11] para asignar claves a nodos Chord. El hashing consistente tiende a equilibrar la carga, ya que cada nodo recibe aproximadamente la misma cantidad de claves, e involucra relativamente poco movimiento de claves cuando los nodos se unen y se van del sistema.

El trabajo previo en hashing consistente asumió que los nodos eran conscientes de la mayoría de los demás nodos en el sistema, lo que lo hacía impráctico para escalar a una gran cantidad de nodos. En contraste, cada nodo Chord solo necesita información de "enrutamiento" sobre unos pocos nodos. Debido a que la tabla de enrutamiento está distribuida, un nodo resuelve la función hash comunicándose con unos pocos nodos. En estado estable, en un sistema de "n" nodos, cada nodo mantiene información solo sobre "O(log n)" otros nodos, y resuelve todas las búsquedas a través de "O(log n)" mensajes a otros nodos. Chord mantiene su información de enrutamiento a medida que los nodos se unen y se van del sistema; con alta probabilidad, cada uno de estos eventos da como resultado no más de "O(log n)" mensajes.

Tres características que distinguen a Chord de muchos otros protocolos de búsqueda de par a par son su simplicidad, corrección demostrable y rendimiento demostrable. Chord es simple, enrutando una clave a través de una secuencia de otros nodos hacia el destino. Un nodo Chord requiere información sobre otros nodos para un enrutamiento eficiente, pero el rendimiento se degrada gradualmente cuando esa información está desactualizada. Esto es importante en la práctica porque los nodos se unirán y se irán arbitrariamente, y la coherencia incluso del estado puede ser difícil de mantener. Solo una pieza de información por nodo debe ser correcta para que Chord garantice el enrutamiento correcto (aunque lento) de las consultas; Chord tiene un algoritmo simple para mantener esta información en un entorno dinámico.

El resto de este documento está estructurado de la siguiente manera. La sección 2 compara Chord con el trabajo relacionado. La sección 3 presenta el modelo del sistema que motiva el protocolo Chord. La sección 4 presenta el protocolo base de Chord y prueba varias de sus propiedades, mientras que la sección 5 presenta extensiones para manejar uniones y fallas concurrentes. La sección 6 demuestra nuestras afirmaciones sobre el rendimiento de Chord a través de la simulación y los experimentos en un prototipo implementado. Finalmente, describimos los elementos para el trabajo futuro en la sección 7 y resumimos nuestras contribuciones en la sección 8.

**2. Trabajo relacionado**

Si bien Chord mapea claves a nodos, los servicios tradicionales de nombres y ubicaciones proporcionan un mapeo directo entre claves y valores. Un valor puede ser una dirección, un documento o un elemento de datos arbitrario. Chord puede implementar fácilmente esta funcionalidad almacenando cada par clave/valor en el nodo al que se mapea esa clave. Por esta razón y para hacer la comparación más clara, el resto de esta sección asume un servicio basado en Chord que mapea claves a valores.

DNS proporciona un mapeo de nombre de host a dirección IP [15]. Chord puede proporcionar el mismo servicio con el nombre que representa la clave y la dirección IP asociada que representa el valor. Chord no requiere servidores especiales, mientras que DNS se basa en un conjunto de servidores raíz especiales. Los nombres DNS están estructurados para reflejar límites administrativos; Chord no impone ninguna estructura de nombres. DNS está especializado en la tarea de encontrar hosts o servicios nombrados, mientras que Chord también se puede utilizar para encontrar objetos de datos que no están vinculados a máquinas particulares.

El sistema de almacenamiento de par a par Freenet [4, 5], al igual que Chord, es descentralizado y simétrico, y se adapta automáticamente cuando los hosts se van y se unen. Freenet no asigna responsabilidad por los documentos a servidores específicos; en cambio, sus búsquedas toman la forma de búsquedas de copias en caché. Esto permite que Freenet proporcione un grado de anonimato, pero le impide garantizar la recuperación de documentos existentes o proporcionar límites bajos en los costos de recuperación. Chord no proporciona anonimato, pero su operación de búsqueda se ejecuta en tiempo predecible y siempre da como resultado el éxito o el fracaso definitivo.

El sistema Ohaha utiliza un algoritmo similar al hashing consistente para asignar documentos a nodos, y enrutamiento de consultas estilo Freenet [18]. Como resultado, comparte algunas de las debilidades de Freenet. Archival Intermemory utiliza un árbol calculado fuera de línea para asignar direcciones lógicas a máquinas que almacenan los datos [3].

El sistema Globe [2] tiene un servicio de ubicación de área amplia para asignar identificadores de objetos a las ubicaciones de objetos en movimiento. Globe organiza Internet como una jerarquía de dominios geográficos, topológicos o administrativos, construyendo efectivamente un árbol de búsqueda estático en todo el mundo, similar a DNS. La información sobre un objeto se almacena en un dominio hoja particular, y las cachés de punteros proporcionan atajos de búsqueda [22]. El sistema Globe maneja la alta carga en la raíz lógica al particionar los objetos entre varios servidores raíz físicos utilizando técnicas de tipo hash. Chord realiza esta función hash lo suficientemente bien como para lograr la escalabilidad sin involucrar ninguna jerarquía, aunque Chord no explota la localidad de la red tan bien como Globe.

El protocolo de ubicación de datos distribuidos desarrollado por Plaxton et al. [19], una variante del cual se utiliza en OceanStore [12], es quizás el algoritmo más cercano al protocolo Chord. Proporciona garantías más fuertes que Chord: al igual que Chord, garantiza que las consultas realicen un número logarítmico de saltos y que las claves estén bien equilibradas, pero el protocolo Plaxton también garantiza, sujeto a supuestos sobre la topología de la red, que las consultas nunca viajan más en distancia de red que el nodo donde se almacena la clave. La ventaja de Chord es que es sustancialmente menos complicado y maneja bien las uniones y fallas de nodos concurrentes. El protocolo Chord también es similar a Pastry, el algoritmo de ubicación utilizado en PAST [8]. Sin embargo, Pastry es un protocolo de enrutamiento basado en prefijos, y difiere en otros detalles de Chord.

CAN utiliza un espacio de coordenadas cartesianas "d" -dimensional (para algún "d" fijo) para implementar una tabla hash distribuida que mapea claves a valores [20]. Cada nodo mantiene "O(d)" estado, y el costo de búsqueda es "O(d)". Por lo tanto, en contraste con Chord, el estado mantenido por un nodo CAN no depende del tamaño de la red "n", pero el costo de búsqueda aumenta más rápido que "log n". Si "d = log n", los tiempos de búsqueda de CAN y las necesidades de almacenamiento coinciden con las de Chord. Sin embargo, CAN no está diseñado para variar "d" cuando "n" (y por lo tanto "log n") varía, por lo que esta coincidencia solo ocurrirá para el "d" correcto correspondiente al "d" fijo. CAN requiere un protocolo de mantenimiento adicional para volver a mapear periódicamente el espacio de identificadores en los nodos. Chord también tiene la ventaja de que su corrección es robusta frente a información de enrutamiento parcialmente incorrecta.

El procedimiento de enrutamiento de Chord se puede considerar como un análogo unidimensional del sistema de ubicación Grid [14]. Grid se basa en información de ubicación geográfica del mundo real para enrutar sus consultas; Chord mapea sus nodos a un espacio unidimensional artificial dentro del cual el enrutamiento se lleva a cabo mediante un algoritmo similar al de Grid.

Chord se puede utilizar como un servicio de búsqueda para implementar una variedad de sistemas, como se discute en la sección 3. En particular, puede ayudar a evitar los puntos únicos de falla o control que poseen sistemas como Napster [17], y la falta de escalabilidad que los sistemas como Gnutella muestran debido a su uso generalizado de transmisiones [10].

**3. Modelo del sistema**

Chord simplifica el diseño de sistemas de par a par y aplicaciones basadas en él al abordar estos problemas difíciles:

* **Equilibrio de carga:** Chord actúa como una función hash distribuida, distribuyendo las claves uniformemente entre los nodos; esto proporciona un grado de equilibrio de carga natural.
* **Descentralización:** Chord está completamente distribuido: ningún nodo es más importante que ningún otro. Esto mejora la robustez y hace que Chord sea apropiado para aplicaciones de par a par poco organizadas.
* **Escalabilidad:** El costo de una búsqueda de Chord crece como el logaritmo de la cantidad de nodos, por lo que incluso los sistemas muy grandes son viables. No se requiere ningún ajuste de parámetros para lograr esta escalabilidad.
* **Disponibilidad:** Chord ajusta automáticamente sus tablas internas para reflejar los nodos recién unidos, así como las fallas de los nodos, asegurando que, salvo fallas importantes en la red subyacente, siempre se pueda encontrar el nodo responsable de una clave. Esto es cierto incluso si el sistema está en un estado continuo de cambio.
* **Nomenclatura flexible:** Chord no impone restricciones en la estructura de las claves que busca: el espacio de claves de Chord es plano. Esto les da a las aplicaciones una gran flexibilidad en cómo mapean sus propios nombres a claves Chord.

El software Chord toma la forma de una biblioteca que se vinculará con las aplicaciones cliente y servidor que lo utilizan. La aplicación interactúa con Chord de dos maneras principales. Primero, Chord proporciona un algoritmo lookup(key) que produce la dirección IP del nodo responsable de la clave. Segundo, el software Chord en cada nodo notifica a la aplicación los cambios en el conjunto de claves de las que el nodo es responsable. Esto permite que el software de la aplicación, por ejemplo, mueva los valores correspondientes a sus nuevos hogares cuando se une un nuevo nodo.

La aplicación que utiliza Chord es responsable de proporcionar cualquier autenticación, almacenamiento en caché, replicación y nomenclatura amigable para el usuario de los datos que se desee. El espacio de claves plano de Chord facilita la implementación de estas características. Por ejemplo, una aplicación podría autenticar datos almacenándolos bajo una clave Chord derivada de un hash criptográfico de los datos. De manera similar, una aplicación podría replicar datos almacenándolos bajo dos claves Chord distintas derivadas del identificador de nivel de aplicación de los datos.

Los siguientes son ejemplos de aplicaciones para las que Chord proporcionaría una buena base:

* **Espejado cooperativo**, como se describe en una propuesta reciente [6]. Imagine un conjunto de desarrolladores de software, cada uno de los cuales desea publicar una distribución. La demanda de cada distribución puede variar drásticamente, desde muy popular justo después de un nuevo lanzamiento hasta relativamente impopular entre lanzamientos. Un enfoque eficiente para esto sería que los desarrolladores reflejaran cooperativamente las distribuciones de los demás. Idealmente, el sistema de espejado equilibraría la carga entre todos los servidores, replicaría y almacenaría en caché los datos, y aseguraría la autenticidad. Tal sistema debería estar completamente descentralizado en interés de la confiabilidad, y porque no hay una administración central natural.
* **Almacenamiento de tiempo compartido** para nodos con conectividad intermitente. Si una persona desea que algunos datos estén siempre disponibles, pero su máquina solo está disponible ocasionalmente, puede ofrecer almacenar los datos de otros mientras están arriba, a cambio de que sus datos se almacenen en otro lugar cuando están abajo. El nombre de los datos puede servir como una clave para identificar el nodo Chord (vivo) responsable de almacenar el elemento de datos en un momento dado. Surgen muchos de los mismos problemas que en la aplicación de espejado cooperativo, aunque el enfoque aquí está en la disponibilidad en lugar del equilibrio de carga.
* **Índices distribuidos** para admitir la búsqueda de palabras clave estilo Gnutella o Napster. Una clave en esta aplicación podría derivarse de las palabras clave deseadas, mientras que los valores podrían ser listas de máquinas que ofrecen documentos con esas palabras clave.
* **Búsqueda combinatoria a gran escala**, como el descifrado de código. En este caso, las claves son soluciones candidatas al problema (como claves criptográficas); Chord mapea estas claves a las máquinas responsables de probarlas como soluciones.

La Figura 1 muestra una posible estructura de software de tres niveles para un sistema de espejo cooperativo. El nivel más alto proporcionaría una interfaz similar a un archivo a los usuarios, incluida la nomenclatura amigable para el usuario y la autenticación. Esta capa de "sistema de archivos" podría implementar directorios y archivos nombrados, asignando operaciones en ellos a operaciones de bloques de nivel inferior. El siguiente nivel, una capa de "almacenamiento de bloques", implementaría las operaciones de bloque. Se encargaría del almacenamiento, el almacenamiento en caché y la replicación de bloques. La capa de almacenamiento de bloques usaría Chord para identificar el nodo responsable de almacenar un bloque, y luego hablaría con el servidor de almacenamiento de bloques en ese nodo para leer o escribir el bloque.

**4. El Protocolo Base de Chord**

El protocolo Chord especifica cómo encontrar las ubicaciones de las claves, cómo se unen los nuevos nodos al sistema y cómo recuperarse de la falla (o salida planificada) de los nodos existentes. Esta sección describe una versión simplificada del protocolo que no maneja uniones o fallas concurrentes. La sección 5 describe mejoras al protocolo base para manejar uniones y fallas concurrentes.

**4.1 Descripción general**

En su esencia, Chord proporciona un cálculo rápido y distribuido de una función hash que mapea claves a nodos responsables de ellas. Utiliza hashing consistente [11, 13], que tiene varias propiedades buenas. Con alta probabilidad, la función hash equilibra la carga (todos los nodos reciben aproximadamente la misma cantidad de claves). También con alta probabilidad, cuando se une (o se va) un nodo "n" de la red, solo una fracción "O(log n)" de las claves se mueve a una ubicación diferente; esto es claramente el mínimo necesario para mantener una carga equilibrada.

Chord mejora la escalabilidad del hashing consistente al evitar el requisito de que cada nodo conozca todos los demás nodos. Un nodo Chord solo necesita una pequeña cantidad de información de "enrutamiento" sobre otros nodos. Debido a que esta información está distribuida, un nodo resuelve la función hash comunicándose con unos pocos nodos. En una red de "n" nodos, cada nodo mantiene información solo sobre "O(log n)" otros nodos, y una búsqueda requiere "O(log n)" mensajes. Chord debe actualizar la información de enrutamiento cuando un nodo se une o sale de la red; una unión o salida requiere "O(log n)" mensajes.

**4.2 Hashing consistente**

La función hash consistente asigna a cada nodo y clave un identificador de "m" bits utilizando una función hash base como SHA-1 [9]. El identificador de un nodo se elige haciendo hash de la dirección IP del nodo, mientras que un identificador de clave se produce haciendo hash de la clave. Usaremos el término "clave" para referirnos tanto a la clave original como a su imagen bajo la función hash, ya que el significado será claro por el contexto. De manera similar, el término "nodo" se referirá tanto al nodo como a su identificador bajo la función hash. La longitud del identificador "m" debe ser lo suficientemente grande como para que la probabilidad de que dos nodos o claves hagan hash al mismo identificador sea insignificante.

El hashing consistente asigna claves a nodos de la siguiente manera. Los identificadores están ordenados en un círculo de identificadores módulo "2^m". La clave "k" se asigna al primer nodo cuyo identificador es igual o sigue (el identificador de) "k" en el espacio de identificadores. Este nodo se denomina nodo sucesor de la clave "k", denotado por "successor(k)". Si los identificadores se representan como un círculo de números de "0" a "2^m - 1", entonces "successor(k)" es el primer nodo en sentido horario desde "k".

La Figura 2 muestra un círculo de identificadores con "m = 3". El círculo tiene tres nodos: "0", "1" y "3". El sucesor del identificador "1" es el nodo "1", por lo que la clave "1" se ubicaría en el nodo "1". De manera similar, la clave "2" se ubicaría en el nodo "3", y la clave "6" en el nodo "0".

El hashing consistente está diseñado para permitir que los nodos entren y salgan de la red con una interrupción mínima. Para mantener el mapeo de hashing consistente cuando un nodo "n" se une a la red, ciertas claves previamente asignadas al sucesor de "n" ahora se asignan a "n". Cuando el nodo "n" sale de la red, todas sus claves asignadas se reasign an a su sucesor. No es necesario que se produzcan otros cambios en la asignación de claves a nodos. En el ejemplo anterior, si un nodo se uniera con el identificador "7", capturaría la clave con el identificador "6" del nodo con el identificador "0".

Los siguientes resultados se demuestran en los documentos que introdujeron el hashing consistente [11, 13]:

**Teorema 1.** Para cualquier conjunto de "n" nodos y "k" claves, con alta probabilidad:

1. Cada nodo es responsable de como máximo "O(k/n)" claves.
2. Cuando un nodo "n" se une o se va de la red, la responsabilidad de "O(k/n)" claves cambia de manos (y solo hacia o desde el nodo que se une o se va).

Cuando el hashing consistente se implementa como se describió anteriormente, el teorema prueba un límite de "O(k/n)". El documento de hashing consistente muestra que "O(k/n)" se puede reducir a una constante arbitrariamente pequeña haciendo que cada nodo ejecute "O(n)" "nodos virtuales", cada uno con su propio identificador. La frase "con alta probabilidad" requiere alguna discusión. Una interpretación simple es que los nodos y las claves se eligen aleatoriamente, lo cual es plausible en un modelo no adversarial del mundo. La distribución de probabilidad es entonces sobre elecciones aleatorias de claves y nodos, y dice que es poco probable que dicha elección aleatoria produzca una distribución desequilibrada. Sin embargo, uno podría preocuparse por un adversario que intencionalmente elige claves para que todas hagan hash al mismo identificador, destruyendo la propiedad de equilibrio de carga. El documento de hashing consistente utiliza "funciones hash 'k' -universales" para proporcionar ciertas garantías incluso en el caso de claves no aleatorias.

En lugar de utilizar una función hash "k" -universal, elegimos utilizar la función SHA-1 estándar como nuestra función hash base. Esto hace que nuestro protocolo sea determinista, por lo que las afirmaciones de "alta probabilidad" ya no tienen sentido. Sin embargo, producir un conjunto de claves que choquen bajo SHA-1 se puede ver, en cierto sentido, como invertir, o "descifrar", la función SHA-1. Se cree que esto es difícil de hacer. Por lo tanto, en lugar de afirmar que nuestros teoremas se mantienen con alta probabilidad, podemos afirmar que se mantienen "basados en supuestos estándar de dureza".

Por simplicidad (principalmente de presentación), prescindimos del uso de nodos virtuales. En este caso, la carga en un nodo puede exceder el promedio en un factor "O(log n)" con alta probabilidad (o en nuestro caso, basado en supuestos estándar de dureza). Una razón para evitar los nodos virtuales es que el número necesario está determinado por la cantidad de nodos en el sistema, lo cual puede ser difícil de determinar. Por supuesto, uno puede optar por utilizar un límite superior a priori en la cantidad de nodos en el sistema; por ejemplo, podríamos postular como máximo un servidor Chord por dirección IPv4. En este caso, ejecutar 32 nodos virtuales por nodo físico proporcionaría un buen equilibrio de carga.

**4.3 Ubicación de claves escalable**

Una cantidad muy pequeña de información de enrutamiento es suficiente para implementar el hashing consistente en un entorno distribuido. Cada nodo solo necesita estar al tanto de su nodo sucesor en el círculo. Las consultas para un identificador dado se pueden pasar alrededor del círculo a través de estos punteros sucesores hasta que encuentren por primera vez un nodo que sucede al identificador; este es el nodo al que se mapea la consulta. Una parte del protocolo Chord mantiene estos punteros sucesores, asegurando así que todas las búsquedas se resuelvan correctamente. Sin embargo, este esquema de resolución es ineficiente: puede requerir recorrer todos los "n" nodos para encontrar el mapeo apropiado. Para acelerar este proceso, Chord mantiene información de enrutamiento adicional. Esta información adicional no es esencial para la corrección, que se logra siempre que la información sucesora se mantenga correctamente.

Como antes, sea "m" la cantidad de bits en los identificadores de clave/nodo. Cada nodo, "n", mantiene una tabla de enrutamiento con (como máximo) "m" entradas, llamada la tabla de dedos. La entrada "i" en la tabla en el nodo "n" contiene la identidad del primer nodo, "finger[i].node", que sucede "n" por al menos "2^(i-1)" en el círculo de identificadores, es decir, "finger[i].node > n + 2^(i-1) (mod 2^m)", donde "finger[i].start = n + 2^(i-1) (mod 2^m)" (y toda la aritmética es módulo "2^m"). Llamamos al nodo "finger[i].node" el "i" -ésimo dedo del nodo "n", y lo denotamos por "finger[i].node" (ver Tabla 1). Una entrada de la tabla de dedos incluye tanto el identificador Chord como la dirección IP (y el número de puerto) del nodo relevante. Tenga en cuenta que el primer dedo de "n" es su sucesor inmediato en el círculo; por conveniencia, a menudo nos referimos a él como el sucesor en lugar del primer dedo.

En el ejemplo que se muestra en la Figura 3(b), la tabla de dedos del nodo "3" apunta a los nodos sucesores de los identificadores "2 (mod 4)", "3 (mod 4)" y "5 (mod 4)", respectivamente. El sucesor del identificador "2" es el nodo "3", ya que este es el primer nodo que sigue a "2", el sucesor del identificador "3" es (trivialmente) el nodo "3", y el sucesor de "5" es el nodo "0". Este esquema tiene dos características importantes. Primero, cada nodo almacena información solo sobre una pequeña cantidad de otros nodos, y sabe más sobre los nodos que lo siguen de cerca en el círculo de identificadores que sobre los nodos que están más lejos. Segundo, la tabla de dedos de un nodo generalmente no contiene suficiente información para determinar el sucesor de una clave arbitraria "k". Por ejemplo, el nodo "3" en la Figura 3 no conoce el sucesor de "1", ya que el sucesor de "1" (nodo "1") no aparece en la tabla de dedos del nodo "3".

¿Qué sucede cuando un nodo "n" no conoce el sucesor de una clave "k"? Si "n" puede encontrar un nodo cuyo ID esté más cerca que el suyo a "k", ese nodo sabrá más sobre el círculo de identificadores en la región de "k" que "n". Por lo tanto, "n" busca en su tabla de dedos el nodo "finger[i].node" cuyo ID precede inmediatamente a "k", y le pregunta a "finger[i].node" por el nodo que conoce cuyo ID esté más cerca de "k". Repitiendo este proceso, "n" aprende sobre nodos con ID más y más cerca de "k".

El pseudocódigo que implementa el proceso de búsqueda se muestra en la Figura 4. La notación "n.foo()" representa la función "foo()" que se invoca y ejecuta en el nodo "n". Las llamadas remotas y las referencias a variables están precedidas por el identificador del nodo remoto, mientras que las referencias a variables locales y las llamadas a procedimientos omiten el nodo local. Por lo tanto, "n.foo()" denota una llamada de procedimiento remoto en el nodo "n", mientras que "n.bar", sin paréntesis, es una RPC para buscar una variable "bar" en el nodo "n".

La función "find successor" funciona encontrando el nodo predecesor inmediato del identificador deseado; el sucesor de ese nodo debe ser el sucesor del identificador. Implementamos "find predecessor" explícitamente, porque se utiliza más adelante para implementar la operación de unión (sección 4.4).

Cuando el nodo "n" ejecuta "find predecessor", contacta a una serie de nodos moviéndose hacia adelante alrededor del círculo Chord hacia "k". Si el nodo "n" contacta a un nodo "p" tal que "k" cae entre "p" y el sucesor de "p", "find predecessor" termina y devuelve "p". De lo contrario, el nodo "n" le pide a "p" que encuentre el nodo que "p" conoce que precede más estrechamente a "k".

Por lo tanto, el algoritmo siempre avanza hacia el predecesor de "k". Como ejemplo, considere el anillo Chord en la Figura 3(b). Suponga que el nodo "3" desea encontrar el sucesor del identificador "1". Dado que "1" pertenece al intervalo circular "finger[3].interval", pertenece al intervalo del tercer dedo; por lo tanto, el nodo "3" verifica la tercera entrada en su tabla de dedos, que es "0". Debido a que "0" precede a "1", el nodo "3" le pedirá al nodo "0" que encuentre el sucesor de "1". A su vez, el nodo "0" inferirá de su tabla de dedos que el sucesor de "1" es el nodo "1" en sí mismo, y devolverá el nodo "1" al nodo "3".

Los punteros de dedos a distancias que se duplican repetidamente alrededor del círculo hacen que cada iteración del bucle en "find predecessor" reduzca a la mitad la distancia al identificador objetivo. De esta intuición se sigue un teorema:

**Teorema 2.** Con alta probabilidad (o bajo supuestos estándar de dureza), la cantidad de nodos con los que se debe contactar para encontrar un sucesor en una red de "n" nodos es "O(log n)".

**Prueba.** Suponga que el nodo "n" desea resolver una consulta para el sucesor de "k". Sea "p" el nodo que precede inmediatamente a "k". Analizamos la cantidad de pasos de consulta para llegar a "p".

Recuerde que si "k ∈ finger[i].interval", entonces "n" reenvía su consulta al predecesor más cercano de "k" en su tabla de dedos. Suponga que el nodo "p" está en el intervalo del "i" -ésimo dedo del nodo "n". Entonces, dado que este intervalo no está vacío, el nodo "n" apuntará a algún nodo "finger[i].node" en este intervalo. La distancia (cantidad de identificadores) entre "finger[i].node" y "p" es al menos "2^(i-1)". Pero "finger[i].node" y "p" están ambos en el intervalo del "i" -ésimo dedo de "n", lo que significa que la distancia entre ellos es como máximo "2^i". Esto significa que "finger[i].node" está más cerca de "p" que de "n", o de manera equivalente, que la distancia de "finger[i].node" a "p" es como máximo la mitad de la distancia de "n" a "p". Si la distancia entre el nodo que maneja la consulta y el predecesor "p" se reduce a la mitad en cada paso, y es como máximo "n" inicialmente, entonces dentro de "log n" pasos la distancia será uno, lo que significa que hemos llegado a "p".

De hecho, como se discutió anteriormente, asumimos que los identificadores de nodos y claves son aleatorios. En este caso, la cantidad de reenvíos necesarios será "O(log n)" con alta probabilidad. Después de "log n" reenvíos, la distancia entre el nodo de consulta actual y la clave "k" se reducirá a como máximo "n/2^(log n)" = "1". La cantidad esperada de identificadores de nodos que caen en un rango de este tamaño es "1", y es "O(1)" con alta probabilidad. Por lo tanto, incluso si los pasos restantes avanzan solo un nodo a la vez, cruzarán todo el intervalo restante y llegarán a la clave "k" dentro de otros "O(1)" pasos.

En la sección que informa nuestros resultados experimentales (sección 6), observaremos (y justificaremos) que el tiempo de búsqueda promedio es "O(log n)".

**4.4 Uniones de nodos**

En una red dinámica, los nodos pueden unirse (y salir) en cualquier momento. El principal desafío en la implementación de estas operaciones es preservar la capacidad de ubicar cada clave en la red. Para lograr este objetivo, Chord debe preservar dos invariantes:

1. El sucesor de cada nodo se mantiene correctamente.
2. Para cada clave "k", el nodo "successor(k)" es responsable de "k".

Para que las búsquedas sean rápidas, también es deseable que las tablas de dedos sean correctas.

Esta sección muestra cómo mantener estas invariantes cuando se une un solo nodo. Diferimos la discusión de múltiples nodos que se unen simultáneamente a la sección 5, que también analiza cómo manejar la falla de un nodo. Antes de describir la operación de unión, resumimos su rendimiento (la prueba de este teorema está en el informe técnico complementario [21]):

**Teorema 3.** Con alta probabilidad, cualquier nodo que se una o se vaya de una red Chord de "n" nodos usará "O(log n)" mensajes para restablecer las invariantes de enrutamiento y las tablas de dedos de Chord.

Para simplificar los mecanismos de unión y salida, cada nodo en Chord mantiene un puntero predecesor. El puntero predecesor de un nodo contiene el identificador Chord y la dirección IP del predecesor inmediato de ese nodo, y se puede usar para caminar en sentido contrario a las agujas del reloj alrededor del círculo de identificadores.

Para preservar las invariantes mencionadas anteriormente, Chord debe realizar tres tareas cuando un nodo "n" se une a la red:

1. Inicializar el predecesor y los dedos del nodo "n".
2. Actualizar los dedos y predecesores de los nodos existentes para reflejar la adición de "n".
3. Notificar al software de capa superior para que pueda transferir el estado (por ejemplo, valores) asociado con las claves de las que ahora es responsable el nodo "n".

Asumimos que el nuevo nodo conoce la identidad de un nodo Chord existente "p" mediante algún mecanismo externo. El nodo "n" utiliza "p" para inicializar su estado y agregarse a la red Chord existente, de la siguiente manera.

**Inicializando dedos y predecesor:** El nodo "n" aprende su predecesor y dedos pidiéndole a "p" que los busque, utilizando el pseudocódigo "init finger table" en la Figura 6. Realizar "find successor" de forma ingenua para cada una de las "m" entradas de dedos daría un tiempo de ejecución de "O(m log n)". Para reducir esto, "n" verifica si el "i" -ésimo dedo también es el "2^(i-1)" -ésimo dedo correcto, para cada "i". Esto sucede cuando el intervalo del "i" -ésimo dedo no contiene ningún nodo, y por lo tanto, "finger[i].node = finger[i].start". Se puede demostrar que el cambio reduce la cantidad esperada (y de alta probabilidad) de entradas de dedos que deben buscarse a "O(log n)", lo que reduce el tiempo total a "O(log n)". Como optimización práctica, un nodo recién unido "n" puede solicitar a un vecino inmediato una copia de su tabla de dedos completa y su predecesor. "n" puede utilizar el contenido de estas tablas como sugerencias para ayudarlo a encontrar los valores correctos para sus propias tablas, ya que las tablas de "n" serán similares a las de sus vecinos. Se puede demostrar que esto reduce el tiempo para llenar la tabla de dedos a "O(log n)".

**Actualizando los dedos de los nodos existentes:** El nodo "n" deberá ingresarse en las tablas de dedos de algunos nodos existentes. Por ejemplo, en la Figura 5(a), el nodo "6" se convierte en el tercer dedo de los nodos "0" y "1", y el primer y el segundo dedo del nodo "3".

La Figura 6 muestra el pseudocódigo de la función "update finger table" que actualiza las tablas de dedos existentes. El nodo "n" se convertirá en el "i" -ésimo dedo del nodo "n" si y solo si (1) "n" precede a "n" por al menos "2^(i-1)", y (2) el "i" -ésimo dedo del nodo "n" sucede a "n". El primer nodo, "p", que puede cumplir estas dos condiciones es el predecesor inmediato de "n + 2^(i-1) (mod 2^m)".

Por lo tanto, para un "i" dado, el algoritmo comienza con el "i" -ésimo dedo del nodo "n", y luego continúa caminando en la dirección contraria a las agujas del reloj en el círculo de identificadores hasta que encuentra un nodo cuyo "i" -ésimo dedo precede a "n". Mostramos en el informe técnico [21] que la cantidad de nodos que deben actualizarse cuando un nodo se une a la red es "O(log n)" con alta probabilidad. Encontrar y actualizar estos nodos toma "O(log n)" tiempo. Un esquema más sofisticado puede reducir este tiempo a "O(1)"; sin embargo, no lo presentamos ya que esperamos que las implementaciones utilicen el algoritmo de la siguiente sección.

**Transfiriendo claves:** La última operación que debe realizarse cuando un nodo "n" se une a la red es transferir la responsabilidad de todas las claves para las cuales el nodo "n" ahora es el sucesor. Exactamente lo que esto implica depende del software de capa superior que utiliza Chord, pero normalmente implicaría mover los datos asociados con cada clave al nuevo nodo. El nodo "n" solo puede convertirse en el sucesor de las claves que eran previamente responsabilidad del nodo que sigue inmediatamente a "n", por lo que "n" solo necesita contactar a ese nodo para transferir la responsabilidad de todas las claves relevantes.

**5. Operaciones concurrentes y fallas**

En la práctica, Chord necesita lidiar con nodos que se unen al sistema de forma concurrente y con nodos que fallan o se van voluntariamente. Esta sección describe modificaciones a los algoritmos básicos de Chord descritos en la sección 4 para manejar estas situaciones.

## Continúa la Traducción del Documento Chord:

**5.1 Estabilización**

El algoritmo de unión en la sección 4 mantiene agresivamente las tablas de dedos de todos los nodos a medida que la red evoluciona. Dado que este invariante es difícil de mantener frente a uniones concurrentes en una red grande, separamos nuestros objetivos de corrección y rendimiento. Se utiliza un protocolo básico de "estabilización" (stabilization) para mantener actualizados los punteros de sucesor de los nodos, lo que es suficiente para garantizar la corrección de las búsquedas. Esos punteros de sucesor se utilizan luego para verificar y corregir las entradas de la tabla de dedos, lo que permite que estas búsquedas sean rápidas y correctas.

Si los nodos que se unen han afectado alguna región del anillo Chord, una búsqueda que se produce antes de que la estabilización haya terminado puede exhibir uno de los tres comportamientos. El caso común es que todas las entradas de la tabla de dedos involucradas en la búsqueda estén razonablemente actualizadas, y la búsqueda encuentre el sucesor correcto en "O(log n)" pasos. El segundo caso es donde los punteros de sucesor son correctos, pero los dedos son inexactos. Esto produce búsquedas correctas, pero pueden ser más lentas. En el último caso, los nodos en la región afectada tienen punteros de sucesor incorrectos, o las claves aún no se han migrado a los nodos recién unidos, y la búsqueda puede fallar. El software de capa superior que utiliza Chord notará que los datos deseados no se encontraron, y tendrá la opción de volver a intentar la búsqueda después de una pausa. Esta pausa puede ser breve, ya que la estabilización corrige los punteros de sucesor rápidamente.

Nuestro esquema de estabilización garantiza que los nodos se agreguen a un anillo Chord de una manera que preserve la capacidad de acceso a los nodos existentes, incluso frente a uniones concurrentes y mensajes perdidos y desordenados. La estabilización por sí sola no corregirá un sistema Chord que se haya dividido en múltiples ciclos disjuntos, o un solo ciclo que recorra varias veces el espacio de identificadores. Estos casos patológicos no se pueden producir mediante ninguna secuencia de uniones de nodos ordinarias. No está claro si se pueden producir mediante particiones de red y recuperaciones o fallas intermitentes. Si se producen, estos casos podrían detectarse y repararse mediante un muestreo periódico de la topología del anillo.

La Figura 7 muestra el pseudocódigo para las uniones y la estabilización; este código reemplaza la Figura 6 para manejar uniones concurrentes. Cuando el nodo "n" inicia por primera vez, llama a "join(p)", donde "p" es cualquier nodo Chord conocido. La función "join" le pide a "p" que encuentre el sucesor inmediato de "n". Por sí mismo, "join" no hace que el resto de la red sea consciente de "n".

Cada nodo ejecuta "stabilize" periódicamente (así es como la red se da cuenta de los nodos recién unidos). Cuando el nodo "n" ejecuta "stabilize", le pide al sucesor de "n" que encuentre el predecesor del sucesor "p", y decide si "p" debería ser el sucesor de "n" en su lugar. Este sería el caso si el nodo "p" se unió recientemente al sistema. "stabilize" también notifica al sucesor del nodo "n" sobre la existencia de "n", dándole al sucesor la oportunidad de cambiar su predecesor a "n". El sucesor solo hace esto si no conoce ningún predecesor más cercano que "n".

Como ejemplo simple, suponga que el nodo "n" se une al sistema, y su ID se encuentra entre los nodos "p" y "s". "n" adquiriría "s" como su sucesor. El nodo "s", cuando se le notifica por "n", adquiriría "n" como su predecesor. Cuando "s" ejecute "stabilize" la próxima vez, le preguntará a "n" por su predecesor (que ahora es "p"); "s" luego adquiriría "p" como su sucesor. Finalmente, "p" notificará a "n", y "n" adquirirá "p" como su predecesor. En este punto, todos los punteros de predecesor y sucesor son correctos.

Tan pronto como los punteros de sucesor sean correctos, las llamadas a "find predecessor" (y por lo tanto a "find successor") funcionarán. Los nodos recién unidos que aún no se han "dedo-ficado" (fingered) pueden hacer que "find predecessor" inicialmente subestime, pero el bucle en el algoritmo de búsqueda, sin embargo, seguirá los punteros de sucesor (dedo) a través de los nodos recién unidos hasta que se alcance el predecesor correcto. Eventualmente, "fix fingers" ajustará las entradas de la tabla de dedos, eliminando la necesidad de estas exploraciones lineales.

Los siguientes teoremas (demostrados en el informe técnico [21]) muestran que todos los problemas causados por las uniones concurrentes son transitorios. Los teoremas asumen que dos nodos cualesquiera que intenten comunicarse eventualmente tendrán éxito.

**Teorema 4.** Una vez que un nodo puede resolver correctamente una consulta determinada, siempre podrá hacerlo en el futuro.

**Teorema 5.** En algún momento después de la última unión, todos los punteros de sucesor serán correctos.

Las pruebas de estos teoremas se basan en un invariante y un argumento de terminación. El invariante establece que una vez que el nodo "n" puede llegar al nodo "p" a través de punteros de sucesor, siempre puede hacerlo. Para argumentar la terminación, consideramos el caso en el que dos nodos creen tener el mismo sucesor "s". En este caso, cada uno intentará notificar a "s", y "s" eventualmente elegirá el más cercano de los dos (o algún otro más cercano) como su predecesor. En este punto, el más lejano de los dos, al contactar a "s", se enterará de un mejor sucesor que "s". Se deduce que cada nodo avanza hacia un sucesor mejor y mejor con el tiempo. Este progreso eventualmente debe detenerse en un estado donde cada nodo se considera el sucesor de exactamente otro nodo; esto define un ciclo (o conjunto de ellos, pero el invariante asegura que habrá como máximo uno).

No hemos discutido el ajuste de los dedos cuando los nodos se unen porque resulta que las uniones no dañan sustancialmente el rendimiento de los dedos. Si un nodo tiene un dedo en cada intervalo, entonces estos dedos aún se pueden usar incluso después de las uniones. El argumento de la reducción de la distancia a la mitad es esencialmente inalterado, mostrando que "O(log n)" saltos son suficientes para alcanzar un nodo "cerca" del objetivo de una consulta. Las nuevas uniones influyen en la búsqueda solo al interponerse entre el predecesor y el sucesor antiguos de una consulta objetivo. Es posible que deba escanear estos nuevos nodos de forma lineal (si sus dedos aún no son precisos). Pero a menos que se una una gran cantidad de nodos al sistema, es probable que la cantidad de nodos entre dos nodos antiguos sea muy pequeña, por lo que el impacto en la búsqueda es insignificante. Formalmente, podemos afirmar lo siguiente:

**Teorema 6.** Si tomamos una red estable con "n" nodos, y otro conjunto de hasta "n" nodos se une a la red sin punteros de dedos (pero con punteros de sucesor correctos), entonces las búsquedas seguirán tomando "O(log n)" tiempo con alta probabilidad.

De manera más general, siempre que el tiempo que lleva ajustar los dedos sea menor que el tiempo que lleva a la red duplicar su tamaño, las búsquedas deberían continuar tomando "O(log n)" saltos.

**5.2 Fallos y Replicación**

Cuando un nodo "n" falla, los nodos cuyas tablas de dedos incluyen "n" deben encontrar el sucesor de "n". Además, la falla de "n" no debe permitirse interrumpir las consultas que están en progreso mientras el sistema se reestabiliza.

El paso clave en la recuperación de fallas es mantener la información correcta de los sucesores, ya que en el peor de los casos, "find predecessor" puede avanzar solo usando la información de los sucesores. Para ayudar a lograr esto, cada nodo Chord mantiene una "lista de sucesores" (successor-list) de sus "n" sucesores más cercanos en el anillo Chord. En operación normal, una versión modificada de la rutina de "estabilización" (stabilize)  en la Figura 7 mantiene la lista de sucesores. Si el nodo detecta que su sucesor ha fallado, lo reemplaza con la primera entrada válida en su lista de sucesores. En ese punto, el nodo puede dirigir las consultas regulares para claves que estaban bajo la responsabilidad del nodo que falló al nuevo sucesor. Con el tiempo, la función "stabilize" corregirá las entradas de la tabla de dedos y las entradas de la lista de sucesores que apuntan al nodo que falló.

Después de que un nodo falle, pero antes de que la estabilización se complete, otros nodos pueden intentar enviar solicitudes a través del nodo que falló como parte de una consulta "find successor". Idealmente, las consultas deberían poder continuar, después de un tiempo de espera, por otra ruta a pesar del fallo. En muchos casos esto es posible. Todo lo que se necesita es una lista de nodos alternativos, fáciles de encontrar en las entradas de la tabla de dedos anteriores a la del nodo que falló. Si el nodo que falló tenía un índice muy bajo en la tabla de dedos, los nodos en la lista de sucesores también están disponibles como alternativas.

El informe técnico demuestra los siguientes dos teoremas que muestran que la lista de sucesores permite que las consultas tengan éxito, y sean eficientes, incluso durante la estabilización [21]:

**Teorema 7.** Si usamos una lista de sucesores de longitud "n" en una red que es inicialmente estable, y luego cada nodo falla con probabilidad 1/2, entonces con alta probabilidad, la función "find successor" devuelve el sucesor vivo más cercano a la clave de consulta.

**Teorema 8.** Si usamos una lista de sucesores de longitud "n" en una red que es inicialmente estable, y luego cada nodo falla con probabilidad 1/2, entonces el tiempo esperado para ejecutar la función "find successor" en la red con fallos es "O(log n)".

La intuición detrás de estas pruebas es sencilla: los "n" sucesores de un nodo fallan con probabilidad "1/2^n", por lo que con alta probabilidad, un nodo estará al tanto de, y podrá reenviar mensajes a, su sucesor vivo más cercano.

El mecanismo de la lista de sucesores también ayuda al software de capa superior a replicar datos. Una aplicación típica que usa Chord podría almacenar réplicas de los datos asociados con una clave en los "n" nodos que suceden a la clave. El hecho de que un nodo Chord realice un seguimiento de sus "n" sucesores significa que puede informar al software de capa superior cuando los sucesores llegan y se van, y por lo tanto, cuándo el software debe propagar nuevas replicas. 

**6. Resultados de la simulación y los experimentos**

En esta sección, evaluamos el protocolo Chord mediante simulación. El simulador utiliza el algoritmo de búsqueda en la Figura 4 y una versión ligeramente anterior de los algoritmos de estabilización descritos en la sección 5. También informamos sobre algunos resultados experimentales preliminares de un sistema basado en Chord en funcionamiento que se ejecuta en hosts de Internet.

**6.1 Simulador del protocolo**

El protocolo Chord se puede implementar en un estilo iterativo o recursivo. En el estilo iterativo, un nodo que resuelve una búsqueda inicia toda la comunicación: solicita información a una serie de nodos de sus tablas de dedos, moviéndose cada vez más cerca en el anillo Chord al sucesor deseado. En el estilo recursivo, cada nodo intermedio reenvía una solicitud al siguiente nodo hasta que llega al sucesor. El simulador implementa los protocolos en un estilo iterativo.

**6.2 Equilibrio de carga**

Primero consideramos la capacidad del hashing consistente para asignar claves a nodos de manera uniforme. En una red con "n" nodos y "k" claves, nos gustaría que la distribución de claves a nodos esté estrechamente relacionada con "k/n".

Consideramos una red que consta de "200" nodos, y variamos la cantidad total de claves de "10.000" a "100.000" en incrementos de "10.000". Para cada valor, repetimos el experimento "20" veces. La Figura 8(a) representa la media y los percentiles "1" y "99" de la cantidad de claves por nodo. La cantidad de claves por nodo exhibe grandes variaciones que aumentan linealmente con la cantidad de claves. Por ejemplo, en todos los casos, algunos nodos no almacenan claves. Para aclarar esto, la Figura 8(b) representa la función de densidad de probabilidad (PDF) de la cantidad de claves por nodo cuando hay "50.000" claves almacenadas en la red. La cantidad máxima de nodos almacenados por cualquier nodo en este caso es "457", o "9,14" la media. Para comparar, el percentil "99" es "22,85" la media.

Una razón para estas variaciones es que los identificadores de nodo no cubren uniformemente todo el espacio de identificadores. Si dividimos el espacio de identificadores en "n" contenedores de igual tamaño, donde "n" es la cantidad de nodos, entonces podríamos esperar ver un nodo en cada contenedor. Pero, de hecho, la probabilidad de que un contenedor en particular no contenga ningún nodo es "((n-1)/n)^n". Para valores grandes de "n", esto se acerca a "e^(-1)".

Como discutimos anteriormente, el documento de hashing consistente resuelve este problema asociando claves a nodos virtuales, y asignando múltiples nodos virtuales (con identificadores no relacionados) a cada nodo real. Intuitivamente, esto proporcionará una cobertura más uniforme del espacio de identificadores. Por ejemplo, si asignamos "O(n)" nodos virtuales elegidos aleatoriamente a cada nodo real, con alta probabilidad, cada uno de los "n" contenedores contendrá "O(1)" nodos [16]. Observamos que esto no afecta la longitud máxima del camino de consulta, que ahora se convierte en "O(log n)" para "O(n)" nodos virtuales. Para verificar esta hipótesis, realizamos un experimento en el que asignamos "n" nodos virtuales a cada nodo real. En este caso, las claves se asocian a nodos virtuales en lugar de nodos reales. Consideramos nuevamente una red con "200" nodos reales y "50.000" claves. La Figura 9 muestra los percentiles "1" y "99" para "n = 10", "n = 100" y "n = 1000", respectivamente. Como se esperaba, el percentil "99" disminuye, mientras que el percentil "1" aumenta con la cantidad de nodos virtuales, "n". En particular, el percentil "99" disminuye de "22,85" a "1,02" la media, mientras que el percentil "1" aumenta de "0" a "0,94" la media. Por lo tanto, agregar nodos virtuales como una capa de indirección puede mejorar significativamente el equilibrio de carga. La compensación es que el uso del espacio de la tabla de enrutamiento aumentará ya que cada nodo real ahora necesita "n" veces más espacio para almacenar las tablas de dedos de sus nodos virtuales. Sin embargo, creemos que este aumento se puede acomodar fácilmente en la práctica. Por ejemplo, suponiendo una red con "200" nodos, y suponiendo "n = 100", cada nodo tiene que mantener una tabla con solo "100 * log(200)" entradas, que es aproximadamente "664" entradas.

**6.3 Longitud del camino**

El rendimiento de cualquier protocolo de enrutamiento depende en gran medida de la longitud del camino entre dos nodos arbitrarios en la red. En el contexto de Chord, definimos la longitud del camino como la cantidad de nodos atravesados durante una operación de búsqueda. Del Teorema 2, con alta probabilidad, la longitud del camino para resolver una consulta es "O(log n)", donde "n" es la cantidad total de nodos en la red.

Para comprender el rendimiento de enrutamiento de Chord en la práctica, simulamos una red con "n" nodos, almacenando "k = 50.000" claves en total. Variamos "n" de "2" a "100.000" y realizamos un experimento separado para cada valor. Cada nodo en un experimento seleccionó un conjunto aleatorio de claves para consultar desde el sistema, y medimos la longitud del camino necesaria para resolver cada consulta.

La Figura 10(a) representa la media y los percentiles "1" y "99" de la longitud del camino como una función de "n". Como se esperaba, la longitud media del camino aumenta logarítmicamente con la cantidad de nodos, al igual que los percentiles "1" y "99". La Figura 10(b) representa la PDF de la longitud del camino para una red con "100" nodos ("k = 50.000").

La Figura 10(a) muestra que la longitud del camino es aproximadamente "log(n) + 2". La razón de los "2" adicionales es la siguiente. Considere un nodo aleatorio y una consulta aleatoria. Considere la distancia en el espacio de identificadores en representación binaria. El bit más significativo (digamos el bit "m") de esta distancia se puede corregir a "0" siguiendo el "m" -ésimo dedo del nodo. Si el siguiente bit significativo de la distancia es "1", también debe corregirse siguiendo un dedo, pero si es "0", entonces no se sigue ningún "m" -ésimo dedo; en cambio, pasamos al "m-1" -ésimo bit. En general, la cantidad de dedos que necesitamos seguir será la cantidad de unos en la representación binaria de la distancia del nodo a la consulta. Dado que la distancia es aleatoria, esperamos que la mitad de los "m" bits sean unos.

**6.4 Fallos simultáneos de nodos**

En este experimento, evaluamos la capacidad de Chord para recuperar la coherencia después de que una gran cantidad de nodos falle simultáneamente. Consideramos nuevamente una red de "200" nodos que almacena "50.000" claves, y seleccionamos aleatoriamente una fracción "f" de los nodos que fallan. Después de que se producen las fallas, esperamos a que la red termine de estabilizarse, y luego medimos la fracción de claves que no se pudieron buscar correctamente. Una búsqueda correcta de una clave es una que encuentra el nodo que originalmente era responsable de la clave, antes de las fallas; esto corresponde a un sistema que almacena valores con claves pero no replica los valores o los recupera después de las fallas.

La Figura 11 representa la tasa media de falla de búsqueda y el intervalo de confianza del "95%" como una función de "f". La tasa de falla de búsqueda es casi exactamente "f". Dado que esta es solo la fracción de claves que se espera que se pierdan debido a la falla de los nodos responsables, concluimos que no hay una falla de búsqueda significativa en la red Chord. Por ejemplo, si la red Chord se hubiera dividido en dos mitades de igual tamaño, esperaríamos que la mitad de las solicitudes fallaran porque el consultante y el destino estarían en particiones diferentes la mitad del tiempo. Nuestros resultados no muestran esto, lo que sugiere que Chord es robusto frente a múltiples fallas simultáneas de nodos.

**6.5 Búsquedas durante la estabilización**

Una búsqueda emitida después de algunas fallas pero antes de que la estabilización se haya completado puede fallar por dos razones. Primero, el nodo responsable de la clave puede haber fallado. Segundo, las tablas de dedos y los punteros de predecesor de algunos nodos pueden ser inconsistentes debido a uniones concurrentes y fallas de nodos. Esta sección evalúa el impacto de las uniones y fallas continuas en las búsquedas.

En este experimento, se considera que una búsqueda ha tenido éxito si llega al sucesor actual de la clave deseada. Esto es ligeramente optimista: en un sistema real, podría haber períodos de tiempo en los que el sucesor real de una clave aún no haya adquirido los datos asociados con la clave del sucesor anterior. Sin embargo, este método nos permite enfocarnos en la capacidad de Chord para realizar búsquedas, en lugar de en la capacidad del software de capa superior para mantener la coherencia de sus propios datos. Cualquier falla de consulta será resultado de inconsistencias en Chord. Además, el simulador no vuelve a intentar las consultas: si una consulta se reenvía a un nodo que está inactivo, la consulta simplemente falla. Por lo tanto, los resultados que se dan en esta sección se pueden ver como el peor de los casos para las fallas de consulta inducidas por la inconsistencia del estado.

Debido a que la principal fuente de inconsistencias son los nodos que se unen y se van, y porque el principal mecanismo para resolver estas inconsistencias es el protocolo de estabilización, el rendimiento de Chord será sensible a la frecuencia de las uniones y salidas de los nodos en relación con la frecuencia con la que se invoca el protocolo de estabilización.

En este experimento, las búsquedas de claves se generan de acuerdo con un proceso de Poisson a una tasa de uno por segundo. Las uniones y fallas se modelan mediante un proceso de Poisson con una tasa media de llegada de "f". Cada nodo ejecuta las rutinas de estabilización a intervalos aleatorios que promedian "30" segundos; a diferencia de las rutinas de la Figura 7, el simulador actualiza todas las entradas de la tabla de dedos en cada invocación. La red comienza con "500" nodos.

La Figura 12 representa las tasas de falla promedio y los intervalos de confianza. Una tasa de falla de nodo de "f = 0,01" corresponde a un nodo que se une y se va cada "100" segundos en promedio. Para comparar, recuerde que cada nodo invoca el protocolo de estabilización una vez cada "30" segundos. En otras palabras, el eje "f" del gráfico varía de una tasa de "1" falla por "3" pasos de estabilización a una tasa de "3" fallas por un paso de estabilización. Los resultados presentados en la Figura 12 se promedian durante aproximadamente dos horas de tiempo simulado. Los intervalos de confianza se calculan a partir de "10" ejecuciones independientes.

Los resultados de la figura 12 se pueden explicar aproximadamente de la siguiente manera. La simulación tiene "500" nodos, lo que significa que las longitudes del camino de búsqueda promedian alrededor de "log(500)" ≈ "9". Una búsqueda falla si su camino de dedo encuentra un nodo que falló.

Si "f*n" nodos fallan, la probabilidad de que uno de ellos esté en el camino del dedo es aproximadamente "f*n/log(n)", o "f * 9" en este caso. Esto sugeriría una tasa de falla de alrededor del "9%" si tenemos "3" fallas entre estabilizaciones. El gráfico muestra resultados en este rango, pero ligeramente peores, ya que puede tomar más de una estabilización para eliminar completamente un nodo que falló.

**6.6 Resultados experimentales**

Esta sección presenta medidas de latencia obtenidas de una implementación prototipo de Chord implementada en Internet. Los nodos Chord están en diez sitios en un subconjunto de la plataforma de prueba RON en los Estados Unidos [1], en California, Colorado, Massachusetts, Nueva York, Carolina del Norte y Pensilvania. El software Chord se ejecuta en UNIX, utiliza claves de "160" bits obtenidas de la función hash criptográfica SHA-1, y utiliza TCP para comunicarse entre nodos. Chord se ejecuta en estilo iterativo. Estos nodos Chord son parte de un sistema de archivos distribuido experimental [7], aunque esta sección considera solo el componente Chord del sistema.

La Figura 13 muestra la latencia medida de las búsquedas de Chord en un rango de cantidades de nodos. Los experimentos con una cantidad de nodos mayor que "10" se llevan a cabo ejecutando múltiples copias independientes del software de nodo Chord en cada sitio. Esto es diferente a ejecutar "O(n)" nodos virtuales en cada sitio para proporcionar un buen equilibrio de carga; más bien, la intención es medir qué tan bien nuestra implementación escala incluso si no tenemos más que una pequeña cantidad de nodos implementados.

Para cada cantidad de nodos que se muestra en la Figura 13, cada sitio físico emite "16" búsquedas de Chord para claves elegidas aleatoriamente una por una. El gráfico representa la mediana, el percentil "5" y el percentil "95" de la latencia de búsqueda. La latencia mediana varía de "180" a "285" ms, dependiendo de la cantidad de nodos. Para el caso de "180" nodos, una búsqueda típica involucra cinco intercambios de mensajes de ida y vuelta: cuatro para la búsqueda de Chord, y un mensaje final al nodo sucesor. Los retrasos de ida y vuelta típicos entre sitios son "60" milisegundos (como se midió mediante ping). Por lo tanto, el tiempo de búsqueda esperado para "180" nodos es de aproximadamente "300" milisegundos, lo que está cerca de la mediana medida de "285". Las bajas latencias del percentil "5" son causadas por búsquedas para claves cerca (en el espacio ID) del nodo de consulta y por saltos de consulta que permanecen locales al sitio físico. Los altos percentiles "95" son causados por búsquedas cuyos saltos siguen rutas de alta demora.

La lección de la Figura 13 es que la latencia de búsqueda crece lentamente con la cantidad total de nodos, confirmando los resultados de la simulación que demuestran la escalabilidad de Chord.

**7. Trabajo futuro**

Basándonos en nuestra experiencia con el prototipo mencionado en la sección 6.6, nos gustaría mejorar el diseño de Chord en las siguientes áreas.

Chord actualmente no tiene ningún mecanismo específico para curar anillos particionados; tales anillos podrían aparecer localmente consistentes para el procedimiento de estabilización. Una forma de verificar la consistencia global es que cada nodo "n" solicite periódicamente a otros nodos que realicen una búsqueda de Chord para "n"; si la búsqueda no produce el nodo "n", puede haber una partición. Esto solo detectará particiones cuyos nodos se conozcan entre sí. Una forma de obtener este conocimiento es que todos los nodos conozcan el mismo pequeño conjunto de nodos iniciales. Otro enfoque podría ser que los nodos mantengan la memoria a largo plazo de un conjunto aleatorio de nodos que han encontrado en el pasado; si se forma una partición, es probable que los conjuntos aleatorios en una partición incluyan nodos de la otra partición.

Un conjunto malicioso o defectuoso de participantes de Chord podría presentar una vista incorrecta del anillo Chord. Asumiendo que los datos que Chord se está utilizando para ubicar están autenticados criptográficamente, esta es una amenaza para la disponibilidad de los datos en lugar de la autenticidad. El mismo enfoque utilizado anteriormente para detectar particiones podría ayudar a las víctimas a darse cuenta de que no están viendo una vista globalmente consistente del anillo Chord.

Un atacante podría apuntar a un elemento de datos en particular insertando un nodo en el anillo Chord con un ID que sigue inmediatamente a la clave del elemento, y haciendo que el nodo devuelva errores cuando se le pide que recupere los datos. Requerir (y verificar) que los nodos utilicen ID derivados del hash SHA-1 de sus direcciones IP hace que este ataque sea más difícil.

Incluso "O(log n)" mensajes por búsqueda pueden ser demasiados para algunas aplicaciones de Chord, especialmente si cada mensaje debe enviarse a un host de Internet aleatorio. En lugar de colocar sus dedos a distancias que son todas potencias de "2", Chord podría cambiarse fácilmente para colocar sus dedos a distancias que son todas potencias enteras de "k". Bajo tal esquema, un solo salto de enrutamiento podría disminuir la distancia a una consulta a "1/k" de la distancia original, lo que significa que "log_k(n)" saltos serían suficientes. Sin embargo, la cantidad de dedos necesarios aumentaría a "log_k(n)". Un enfoque diferente para mejorar la latencia de búsqueda podría ser utilizar la selección de servidor. Cada entrada de la tabla de dedos podría apuntar a los primeros "k" nodos en el intervalo de esa entrada en el anillo ID, y un nodo podría medir el retraso de red a cada uno de los "k" nodos. Los "k" nodos son generalmente equivalentes para los fines de la búsqueda, por lo que un nodo podría reenviar búsquedas al que tenga menor demora. Este enfoque sería más efectivo con búsquedas de Chord recursivas, en las que el nodo que mide los retrasos también es el nodo que reenvía la búsqueda.

**8. Conclusión**

Muchas aplicaciones distribuidas de par a par necesitan determinar el nodo que almacena un elemento de datos. El protocolo Chord resuelve este desafiante problema de manera descentralizada. Ofrece una primitiva poderosa: dada una clave, determina el nodo responsable de almacenar el valor de la clave, y lo hace de manera eficiente. En estado estable, en una red de "n" nodos, cada nodo mantiene información de enrutamiento solo para aproximadamente "O(log n)" otros nodos, y resuelve todas las búsquedas a través de "O(log n)" mensajes a otros nodos. Las actualizaciones de la información de enrutamiento para los nodos que se van y se unen requieren solo "O(log n)" mensajes.

Las características atractivas de Chord incluyen su simplicidad, corrección demostrable y rendimiento demostrable incluso frente a llegadas y salidas concurrentes de nodos. Continúa funcionando correctamente, aunque con un rendimiento degradado, cuando la información de un nodo solo es parcialmente correcta. Nuestro análisis teórico, simulaciones y resultados experimentales confirman que Chord escala bien con la cantidad de nodos, se recupera de una gran cantidad de fallas y uniones de nodos simultáneas, y responde correctamente a la mayoría de las búsquedas incluso durante la recuperación.

Creemos que Chord será un componente valioso para aplicaciones distribuidas de par a par a gran escala como el intercambio de archivos cooperativo, los sistemas de almacenamiento disponibles compartidos en el tiempo, los índices distribuidos para la detección de documentos y servicios, y las plataformas informáticas distribuidas a gran escala.

**Agradecimientos**

Agradecemos a Frank Dabek por las mediciones del prototipo de Chord descritas en la sección 6.6, y a David Andersen por configurar la plataforma de prueba utilizada en esas mediciones.

**9. Referencias**

[1] ANDERSEN, D. Redes superpuestas resilientes. Tesis de maestría, Departamento de EECS, MIT, mayo de 2001. http://nms.lcs.mit.edu/projects/ron/.

[2] BAKKER, A., AMADE, E., BALLINTIJN, G., KUZ, I., VERKAIK, P., VAN DER WIJK, I., VAN STEEN, M., AND TANENBAUM., A. La red de distribución Globe. En Proc. 2000 USENIX Annual Conf. (FREENIX Track) (San Diego, CA, junio de 2000), pp. 141–152.

[3] CHEN, Y., EDLER, J., GOLDBERG, A., GOTTLIEB, A., SOBTI, S., AND YIANILOS, P. Una implementación prototipo de memoria de archivo intermemoria. En Actas de la 4ª Conferencia ACM sobre bibliotecas digitales (Berkeley, CA, agosto de 1999), pp. 28–37.

[4] CLARKE, I. Un sistema distribuido descentralizado de almacenamiento y recuperación de información. Tesis de maestría, Universidad de Edimburgo, 1999.

[5] CLARKE, I., SANDBERG, O., WILEY, B., AND HONG, T. W. Freenet: Un sistema distribuido anónimo de almacenamiento y recuperación de información. En Actas del Taller ICSI sobre Problemas de Diseño en Anonimato e Inobservabilidad (Berkeley, California, junio de 2000). http://freenet.sourceforge.net.

[6] DABEK, F., BRUNSKILL, E., KAASHOEK, M. F., KARGER, D., MORRIS, R., STOICA, I., AND BALAKRISHNAN, H. Construyendo sistemas de par a par con Chord, un servicio de ubicación distribuido. En Actas del 8º Taller IEEE sobre Temas Candentes en Sistemas Operativos (HotOS-VIII) (Elmau/Oberbayern, Alemania, mayo de 2001), pp. 71–76.

[7] DABEK, F., KAASHOEK, M. F., KARGER, D., MORRIS, R., AND STOICA, I. Almacenamiento cooperativo de área amplia con CFS. En Actas del 18º Simposio ACM sobre Principios de Sistemas Operativos (SOSP ’01) (Para aparecer; Banff, Canadá, octubre de 2001).

[8] DRUSCHEL, P., AND ROWSTRON, A. PAST: Almacenamiento persistente y anónimo en un entorno de red de par a par. En Actas del 8º Taller IEEE sobre Temas Candentes en Sistemas Operativos (HotOS 2001) (Elmau/Oberbayern, Alemania, mayo de 2001), pp. 65–70.

[9] FIPS 180-1. Estándar de Hash Seguro. Departamento de Comercio de EE. UU./NIST, Servicio Nacional de Información Técnica, Springfield, VA, abril de 1995.

[10] Gnutella. http://gnutella.wego.com/.

[11] KARGER, D., LEHMAN, E., LEIGHTON, F., LEVINE, M., LEWIN, D., AND PANIGRAHY, R. Hashing consistente y árboles aleatorios: Protocolos de almacenamiento en caché distribuidos para aliviar los puntos calientes en la World Wide Web. En Actas del 29º Simposio Anual ACM sobre Teoría de la Computación (El Paso, TX, mayo de 1997), pp. 654–663.

[12] KUBIATOWICZ, J., BINDEL, D., CHEN, Y., CZERWINSKI, S., EATON, P., GEELS, D., GUMMADI, R., RHEA, S., WEATHERSPOON, H., WEIMER, W., WELLS, C., AND ZHAO, B. OceanStore: Una arquitectura para almacenamiento persistente a escala global. En Actas de la Novena Conferencia Internacional sobre Soporte Arquitectónico para Lenguajes de Programación y Sistemas Operativos (ASPLOS 2000) (Boston, MA, noviembre de 2000), pp. 190–201.

[13] LEWIN, D. Hashing consistente y árboles aleatorios: Algoritmos para almacenar en caché en redes distribuidas. Tesis de maestría, Departamento de EECS, MIT, 1998. Disponible en la Biblioteca del MIT, http://thesis.mit.edu/.

[14] LI, J., JANNOTTI, J., DE COUTO, D., KARGER, D., AND MORRIS, R. Un servicio de ubicación escalable para el enrutamiento geográfico ad hoc. En Actas de la 6ª Conferencia Internacional ACM sobre Computación e Interconexión Móvil (Boston, Massachusetts, agosto de 2000), pp. 120–130.

[15] MOCKAPETRIS, P., AND DUNLAP, K. J. Desarrollo del Sistema de Nombres de Dominio. En Proc. ACM SIGCOMM (Stanford, CA, 1988), pp. 123–133.

[16] MOTWANI, R., AND RAGHAVAN, P. Algoritmos Aleatorios. Cambridge University Press, Nueva York, NY, 1995.

[17] Napster. http://www.napster.com/.

[18] Ohaha, Compartir inteligente descentralizado de par a par. http://www.ohaha.com/design.html.

[19] PLAXTON, C., RAJARAMAN, R., AND RICHA, A. Accediendo a copias cercanas de objetos replicados en un entorno distribuido. En Actas del ACM SPAA (Newport, Rhode Island, junio de 1997), pp. 311–320.

[20] RATNASAMY, S., FRANCIS, P., HANDLEY, M., KARP, R., AND SH